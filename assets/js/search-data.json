{
  
    
        "post0": {
            "title": "Five pytorch function",
            "content": "# Import torch and other required modules import torch . Function 1 - torch.clamp() . Clamp all elements in input into the range [ min, max ] and return tensor . # Example 1 - working i = torch.randn(7) torch.clamp(i, min=-0.5, max=0.5) . tensor([ 0.3991, -0.5000, 0.4462, -0.4993, 0.5000, 0.5000, 0.2525]) . This function retricts minimum and maximum value of elements in tensor. in above example we have put minimum as -0.5 and maximum as 0.5 . # Example 2 - working torch.clamp(i, min=0.0) . tensor([0.3991, 0.0000, 0.4462, 0.0000, 1.6489, 0.9907, 0.2525]) . The above is nothing but a function of relu. relu replaces minimum with zero same as the above example . # Example 3 - breaking (to illustrate when it breaks) torch.clamp(i) . RuntimeError Traceback (most recent call last) &lt;ipython-input-16-bf99f0e86d17&gt; in &lt;module&gt; 1 # Example 3 - breaking (to illustrate when it breaks) -&gt; 2 torch.clamp(i) RuntimeError: At least one of &#39;min&#39; or &#39;max&#39; must not be None . this needs a range without which function breaks . Relu can be implemented using clamp function . Function 2 - torch.argmax() . returns indices of maximum value of all elements in input tensor this function is simple but can be used in finding accuracy for single label classification by passing prediction but before using manual_seed is important to set seed and get same output everytime, it is useful while demonstrating . # Example 1 - working (change this) torch.manual_seed(49) k=torch.randn(7,7) print(k) torch.argmax(k) . tensor([[ 0.2705, -0.3641, 0.5421, 0.1219, 0.5471, -1.1156, 0.5146], [ 0.5792, -0.1513, -0.7178, 0.5251, 2.2830, 0.0806, 1.1384], [-0.5584, -0.4422, 0.0927, 0.1392, -0.9433, 0.6335, -0.2762], [-0.7085, -0.8226, -0.2340, 0.3303, 1.0855, 0.5016, -0.8041], [ 1.6240, 1.5190, -1.2851, -2.4165, -0.3303, 0.6343, -1.5740], [-0.7344, -0.2683, -0.3083, 0.8369, 0.6258, 1.2411, -1.2252], [ 0.3188, 0.6634, 0.2450, 0.1627, 0.8132, 0.2792, -0.2150]]) . tensor(11) . returns the index of maximum value in tensor k. here index of maximum value 1.5190 is 11 . # Example 2 - working print(torch.argmax(k,dim=1)) print(torch.argmax(k,dim=1,keepdim=True)) torch.argmax(k,dim=0,keepdim=True) . tensor([4, 4, 5, 4, 0, 5, 4]) tensor([[4], [4], [5], [4], [0], [5], [4]]) . tensor([[4, 4, 0, 5, 1, 5, 1]]) . here dim=1 means max value is calculated along dim 1 that is row, if dim=0 along column keepdim means whether the output tensor has dim retained or not. . # Example 3 - breaking (to illustrate when it breaks) torch.argmax(torch.tensor([[4.,6.],[8.,10.,12.],[14.,16.]])) . ValueError Traceback (most recent call last) &lt;ipython-input-7-e55fa7412cd2&gt; in &lt;module&gt; 1 # Example 3 - breaking (to illustrate when it breaks) -&gt; 2 torch.argmax(torch.tensor([[4.,6.],[8.,10.,12.],[14.,16.]])) ValueError: expected sequence of length 2 at dim 1 (got 3) . argmax gives error when we give invalid tensor, or tensor of not dimention dimention . This function is used to find maximum value index This function is simple but can be used in finding accuracy for single label classification by passing prediction to function. In final layer when it gives probabilities, this function can be used. . Function 3 - torch.where() . returns tensor of elements selected depending on condition provided. This function is mostly helpful. for example if we have to remove all negative values in tensor with 0 as in case of relu we can use where. . # Example 1 - working x = torch.randn(2, 6) y = torch.zeros(2, 6) torch.where(x &gt; 0, x, y) . tensor([[0.0000, 2.0267, 0.1806, 0.0040, 0.0000, 0.3850], [0.1064, 0.0000, 0.0000, 0.1939, 0.0000, 0.1403]]) . negetive values in tensor x is replaced with 0 from tensor y. . # Example 2 - working x = torch.Tensor([1., 2, 3, 4, 7]) torch.where(x == 7, torch.Tensor([0]), x) . tensor([1., 2., 3., 4., 0.]) . we can use where function to replace a perticular value in tensor . # Example 3 - breaking (to illustrate when it breaks) x = torch.Tensor([1., 2, 3, 4, 7]) torch.where(x == 7, 0, x) . TypeError Traceback (most recent call last) &lt;ipython-input-11-d9404139b323&gt; in &lt;module&gt; 2 x = torch.Tensor([1., 2, 3, 4, 7]) 3 -&gt; 4 torch.where(x == 7, 0, x) TypeError: where(): argument &#39;input&#39; (position 2) must be Tensor, not int . position 2 is tensor, but we have passed int, so failed. so pass tensor in argument . Function 4 - torch.from_numpy() . can create tensor from numpy array. which is mostly useful to run tensor on gpu . # Example 1 - working import numpy as np r = np.array([1,2,3,4,5,6]) a = torch.from_numpy(r) print(a) type(a) . tensor([1, 2, 3, 4, 5, 6]) . torch.Tensor . converts numpy array to tensor . # Example 2 - working l = np.array([111,12,13,14,15,16]) m = torch.from_numpy(l) print(l) type(m) . [111 12 13 14 15 16] . torch.Tensor . the need of this funtion is to convert from numpy nd array to tensor . # Example 3 - breaking (to illustrate when it breaks) g = [7,7,7] h = torch.from_numpy(g) . TypeError Traceback (most recent call last) &lt;ipython-input-22-f48bcc71eb26&gt; in &lt;module&gt; 1 # Example 3 - breaking (to illustrate when it breaks) 2 g = [7,7,7] -&gt; 3 h = torch.from_numpy(g) TypeError: expected np.ndarray (got list) . given anything else then ndarray this function throws error. . the need of this funtion is to convert from numpy nd array to tensor. but this mostly usefull because using numpy we can do operation in cpu and when need convert to tensor and run on gpu . Function 5 - torch.matmul() . this is obviously useful and much need function in neural nets. this function returns matrix product of tensors. . # Example 1 - working x = torch.randn(2, 2) y = torch.randn(2, 5) torch.matmul(x, y) . tensor([[-0.5585, -0.4268, -0.1464, 0.2729, -0.4232], [-0.1942, 2.7557, 1.4192, -0.5796, 0.8051]]) . mat mul of 22 and 25 to give 2*5 . # Example 2 - working x = torch.randn(3, 4, 4) y = torch.randn(3,4,4) torch.matmul(x,y) . tensor([[[ 3.2719, 0.6905, 0.2168, -1.3025], [ 0.8261, -0.0656, -0.1460, -1.4672], [-0.3187, -2.0030, -2.8979, 2.2226], [-2.9364, -0.5917, -1.0406, -0.4551]], [[ 0.7714, 0.6768, -3.8320, 0.4671], [-0.1203, -3.0586, 2.9324, -2.5221], [-0.6227, 0.0743, 0.9032, 0.0845], [-1.5827, 2.8724, 8.0832, 1.3175]], [[-1.2030, 1.9033, 0.7302, -1.7191], [-2.9976, -4.6867, -2.1513, -1.4554], [-0.4289, 1.3620, 0.5602, 0.9934], [-1.7558, -2.1634, 0.2784, 0.0987]]]) . this example is shown because we will have tensors of dimention channelweidthbreadth and to multiply those is useful . # Example 3 - breaking (to illustrate when it breaks) y = torch.randn(3, 1) torch.matmul(x, y) . RuntimeError Traceback (most recent call last) &lt;ipython-input-32-75ff4103ae09&gt; in &lt;module&gt; 1 # Example 3 - breaking (to illustrate when it breaks) 2 y = torch.randn(3, 1) -&gt; 3 torch.matmul(x, y) RuntimeError: size mismatch, m1: [12 x 4], m2: [3 x 1] at /opt/conda/conda-bld/pytorch_1587428266983/work/aten/src/TH/generic/THTensorMath.cpp:41 . column of matrix 1 should match with row of matrix 2 or throws error. . matrix multiplication is main operation in nueral nets. so this is definatly in my list . Conclusion . And that is the end of this exploration of the functions in the PyTorch tensor library. . Reference Links . Provide links to your references and other interesting articles about tensors . Official documentation for torch.Tensor: https://pytorch.org/docs/stable/tensors.html | . !pip install jovian --upgrade --quiet . import jovian . jovian.commit() . [jovian] Attempting to save notebook.. [jovian] Please enter your API key ( from https://jovian.ml/ ): API KEY: ········ [jovian] Updating notebook &#34;kirankamatmgm/01-tensor-operations&#34; on https://jovian.ml/ [jovian] Uploading notebook.. [jovian] Capturing environment.. [jovian] Committed successfully! https://jovian.ml/kirankamatmgm/01-tensor-operations . &#39;https://jovian.ml/kirankamatmgm/01-tensor-operations&#39; .",
            "url": "https://kirankamatmgm.github.io/fastblog/2020/06/12/Five-Useful-pytorch-functions.html",
            "relUrl": "/2020/06/12/Five-Useful-pytorch-functions.html",
            "date": " • Jun 12, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Build and Evaluate a Linear Risk model",
            "content": "Building a risk score model for retinopathy in diabetes patients using logistic regression. . Steps . Data preprocessing Log transformations | Standardization | . | Basic Risk Models Logistic Regression | C-index | Interactions Terms | . | . Diabetic Retinopathy . Retinopathy is an eye condition that causes changes to the blood vessels in the part of the eye called the retina. This often leads to vision changes or blindness. Diabetic patients are known to be at high risk for retinopathy. . Logistic Regression . Logistic regression is used for predicting the probability of a binary outcome. In our case, this would be the probability of having or not having diabetic retinopathy. . import numpy as np import pandas as pd import matplotlib.pyplot as plt . . 1. Load Data . #X_data and y_data are generated files taken from Ai in medicine course X = pd.read_csv(&#39;X_data.csv&#39;,index_col=0) y_df = pd.read_csv(&#39;y_data.csv&#39;,index_col=0) y = y_df[&#39;y&#39;] . X and y are Pandas DataFrames that hold the data for 6,000 diabetic patients. . . 2. Explore the Dataset . The features (X) include the following fields: . Age: (years) | Systolic_BP: Systolic blood pressure (mmHg) | Diastolic_BP: Diastolic blood pressure (mmHg) | Cholesterol: (mg/DL) | . X.head() . Age Systolic_BP Diastolic_BP Cholesterol . 0 77.196340 | 85.288742 | 80.021878 | 79.957109 | . 1 63.529850 | 99.379736 | 84.852361 | 110.382411 | . 2 69.003986 | 111.349455 | 109.850616 | 100.828246 | . 3 82.638210 | 95.056128 | 79.666851 | 87.066303 | . 4 78.346286 | 109.154591 | 90.713220 | 92.511770 | . The target (y) is an indicator of whether or not the patient developed retinopathy. . y = 1 : patient has retinopathy. | y = 0 : patient does not have retinopathy. | . y.head() . 0 1.0 1 1.0 2 1.0 3 1.0 4 1.0 Name: y, dtype: float64 . split the data into train and test sets using a 75/25 split. . For this, we can use the built in function provided by sklearn library. See the documentation for sklearn.model_selection.train_test_split. . from sklearn.model_selection import train_test_split . X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=0) . Plot the histograms of each column of X_train below: . for col in X.columns: X_train_raw.loc[:, col].hist() plt.title(col) plt.show() . As we can see, the distributions have a generally bell shaped distribution, but with slight rightward skew. . We can transform our data to be closer to a normal distribution by removing the skew. One way to remove the skew is by applying the log function to the data. . Let&#39;s plot the log of the feature variables to see that it produces the desired effect. . for col in X_train_raw.columns: np.log(X_train_raw.loc[:, col]).hist() plt.title(col) plt.show() . We can see that the data is more symmetric after taking the log. . . 3. Mean-Normalize the Data . Let&#39;s now transform our data so that the distributions are closer to standard normal distributions. . standardizes the distribution so that for each data point $x$, $$ overline{x} = frac{x - mean(x)}{std(x)}$$ . Pretend that the test data is &quot;unseen&quot; data. This implies that it is unavailable to us for the purpose of preparing our data, and so we do not want to consider it when evaluating the mean and standard deviation that we use in the above equation. Instead we want to calculate these values using the training data alone, but then use them for standardizing both the training and the test data. | . | . def make_standard_normal(df_train, df_test): &quot;&quot;&quot; Args: df_train (dataframe): unnormalized training data. df_test (dataframe): unnormalized test data. Returns: df_train_normalized (dateframe): normalized training data. df_test_normalized (dataframe): normalized test data. &quot;&quot;&quot; df_train_unskewed = np.log(df_train) df_test_unskewed = np.log(df_test) mean = df_train_unskewed.mean(axis=0) stdev = df_train_unskewed.std(axis=0,ddof=1) df_train_standardized = (df_train_unskewed - mean)/stdev df_test_standardized = (df_test_unskewed - mean)/stdev return df_train_standardized, df_test_standardized . Transform training and test data . Use the function to make the data distribution closer to a standard normal distribution. . X_train, X_test = make_standard_normal(X_train_raw, X_test_raw) . After transforming the training and test sets, we&#39;ll expect the training set to be centered at zero with a standard deviation of $1$. . We will avoid observing the test set during model training in order to avoid biasing the model training process, but let&#39;s have a look at the distributions of the transformed training data. . for col in X_train.columns: X_train[col].hist() plt.title(col) plt.show() . . 4. Build the Model . Now we are ready to build the risk model by training logistic regression with our data. . def lr_model(X_train, y_train): from sklearn.linear_model import LogisticRegression model = LogisticRegression(solver=&#39;lbfgs&#39;) model.fit(X_train,y_train) return model . model_X = lr_model(X_train, y_train) . . 5. Evaluate the Model Using the C-index . Now that we have a model, we need to evaluate it. We&#39;ll do this using the c-index. . The c-index measures the discriminatory power of a risk score. | Intuitively, a higher c-index indicates that the model&#39;s prediction is in agreement with the actual outcomes of a pair of patients. | The formula for the c-index is | . $$ mbox{cindex} = frac{ mbox{concordant} + 0.5 times mbox{ties}}{ mbox{permissible}} $$ . A permissible pair is a pair of patients who have different outcomes. | A concordant pair is a permissible pair in which the patient with the higher risk score also has the worse outcome. | A tie is a permissible pair where the patients have the same risk score. | . def cindex(y_true, scores): &#39;&#39;&#39; Input: y_true (np.array): a 1-D array of true binary outcomes (values of zero or one) 0: patient does not get the disease 1: patient does get the disease scores (np.array): a 1-D array of corresponding risk scores output by the model Output: c_index (float): (concordant pairs + 0.5*ties) / number of permissible pairs &#39;&#39;&#39; n = len(y_true) assert len(scores) == n concordant = 0 permissible = 0 ties = 0 for i in range(n): for j in range (i+1, n): # Check if the pair is permissible (the patient outcomes are different) if y_true[i]!=y_true[j]: # Count the pair if it&#39;s permissible permissible +=1 # For permissible pairs, check if they are concordant or are ties # check for ties in the score if scores[i]==scores[j]: # count the tie ties+=1 # if it&#39;s a tie, we don&#39;t need to check patient outcomes, continue to the top of the for loop. continue # case 1: patient i doesn&#39;t get the disease, patient j does if y_true[i] == 0 and y_true[j] == 1: # Check if patient i has a lower risk score than patient j if scores[i]&lt;scores[j]: # count the concordant pair concordant+=1 # Otherwise if patient i has a higher risk score, it&#39;s not a concordant pair. # Already checked for ties earlier # case 2: patient i gets the disease, patient j does not if y_true[i]==1 and y_true[j] == 0: # Check if patient i has a higher risk score than patient j if scores[i]&gt;scores[j]: #count the concordant pair concordant+=1 # Otherwise if patient i has a lower risk score, it&#39;s not a concordant pair. # We already checked for ties earlier # calculate the c-index using the count of permissible pairs, concordant pairs, and tied pairs. c_index = (concordant+(0.5*ties))/permissible return c_index . . 6. Evaluate the Model on the Test Set . Now, you can evaluate your trained model on the test set. . To get the predicted probabilities, we use the predict_proba method. This method will return the result from the model before it is converted to a binary 0 or 1. For each input case, it returns an array of two values which represent the probabilities for both the negative case (patient does not get the disease) and positive case (patient the gets the disease). . scores = model_X.predict_proba(X_test)[:, 1] c_index_X_test = cindex(y_test.values, scores) print(f&quot;c-index on test set is {c_index_X_test:.4f}&quot;) . c-index on test set is 0.8182 . Let&#39;s plot the coefficients to see which variables (patient features) are having the most effect. You can access the model coefficients by using model.coef_ . coeffs = pd.DataFrame(data = model_X.coef_, columns = X_train.columns) coeffs.T.plot.bar(legend=None); . . 7. Improve the Model . You can try to improve your model by including interaction terms. . An interaction term is the product of two variables. For example, if we have data $$ x = [x_1, x_2]$$ | We could add the product so that: $$ hat{x} = [x_1, x_2, x_1*x_2]$$ | . | . def add_interactions(X): &quot;&quot;&quot; Add interaction terms between columns to dataframe. Args: X (dataframe): Original data Returns: X_int (dataframe): Original data with interaction terms appended. &quot;&quot;&quot; features = X.columns m = len(features) X_int = X.copy(deep=True) # &#39;i&#39; loops through all features in the original dataframe X for i in range(m): # get the name of feature &#39;i&#39; feature_i_name = features[i] # get the data for feature &#39;i&#39; feature_i_data = X[feature_i_name] # choose the index of column &#39;j&#39; to be greater than column i for j in range (i+1, m): # get the name of feature &#39;j&#39; feature_j_name = features[j] # get the data for feature j&#39; feature_j_data = X[feature_j_name] # create the name of the interaction feature by combining both names # example: &quot;apple&quot; and &quot;orange&quot; are combined to be &quot;apple_x_orange&quot; feature_i_j_name = f&quot;{feature_i_name}_x_{feature_j_name}&quot; # Multiply the data for feature &#39;i&#39; and feature &#39;j&#39; # store the result as a column in dataframe X_int X_int[feature_i_j_name] = X_int[feature_i_name] * X_int[feature_j_name] return X_int . print(&quot;Original Data&quot;) print(X_train.loc[:, [&#39;Age&#39;, &#39;Systolic_BP&#39;]].head()) print(&quot;Data w/ Interactions&quot;) print(add_interactions(X_train.loc[:, [&#39;Age&#39;, &#39;Systolic_BP&#39;]].head())) . Original Data Age Systolic_BP 1824 -0.912451 -0.068019 253 -0.302039 1.719538 1114 2.576274 0.155962 3220 1.163621 -2.033931 2108 -0.446238 -0.054554 Data w/ Interactions Age Systolic_BP Age_x_Systolic_BP 1824 -0.912451 -0.068019 0.062064 253 -0.302039 1.719538 -0.519367 1114 2.576274 0.155962 0.401800 3220 1.163621 -2.033931 -2.366725 2108 -0.446238 -0.054554 0.024344 . Once you have correctly implemented add_interactions, use it to make transformed version of X_train and X_test. . X_train_int = add_interactions(X_train) X_test_int = add_interactions(X_test) . . 8. Evaluate the Improved Model . Now we can train the new and improved version of the model. . model_X_int = lr_model(X_train_int, y_train) . Let&#39;s evaluate our new model on the test set. . scores_X = model_X.predict_proba(X_test)[:, 1] c_index_X_int_test = cindex(y_test.values, scores_X) scores_X_int = model_X_int.predict_proba(X_test_int)[:, 1] c_index_X_int_test = cindex(y_test.values, scores_X_int) print(f&quot;c-index on test set without interactions is {c_index_X_test:.4f}&quot;) print(f&quot;c-index on test set with interactions is {c_index_X_int_test:.4f}&quot;) . c-index on test set without interactions is 0.8182 c-index on test set with interactions is 0.8281 . You should see that the model with interaction terms performs a bit better than the model without interactions. . Now let&#39;s take another look at the model coefficients to try and see which variables made a difference. Plot the coefficients and report which features seem to be the most important. . int_coeffs = pd.DataFrame(data = model_X_int.coef_, columns = X_train_int.columns) int_coeffs.T.plot.bar(); . You may notice that Age, Systolic_BP, and Cholesterol have a positive coefficient. This means that a higher value in these three features leads to a higher prediction probability for the disease. You also may notice that the interaction of Age x Cholesterol has a negative coefficient. This means that a higher value for the Age x Cholesterol product reduces the prediction probability for the disease. . To understand the effect of interaction terms, let&#39;s compare the output of the model we&#39;ve trained on sample cases with and without the interaction. . index = index = 3432 case = X_train_int.iloc[index, :] print(case) . Age 2.502061 Systolic_BP 1.713547 Diastolic_BP 0.268265 Cholesterol 2.146349 Age_x_Systolic_BP 4.287400 Age_x_Diastolic_BP 0.671216 Age_x_Cholesterol 5.370296 Systolic_BP_x_Diastolic_BP 0.459685 Systolic_BP_x_Cholesterol 3.677871 Diastolic_BP_x_Cholesterol 0.575791 Name: 5970, dtype: float64 . We can see that they have above average Age and Cholesterol. We can now see what our original model would have output by zero-ing out the value for Cholesterol and Age. . new_case = case.copy(deep=True) new_case.loc[&quot;Age_x_Cholesterol&quot;] = 0 new_case . Age 2.502061 Systolic_BP 1.713547 Diastolic_BP 0.268265 Cholesterol 2.146349 Age_x_Systolic_BP 4.287400 Age_x_Diastolic_BP 0.671216 Age_x_Cholesterol 0.000000 Systolic_BP_x_Diastolic_BP 0.459685 Systolic_BP_x_Cholesterol 3.677871 Diastolic_BP_x_Cholesterol 0.575791 Name: 5970, dtype: float64 . print(f&quot;Output with interaction: t{model_X_int.predict_proba([case.values])[:, 1][0]:.4f}&quot;) print(f&quot;Output without interaction: t{model_X_int.predict_proba([new_case.values])[:, 1][0]:.4f}&quot;) . Output with interaction: 0.9448 Output without interaction: 0.9965 . We see that the model is less confident in its prediction with the interaction term than without (the prediction value is lower when including the interaction term). With the interaction term, the model has adjusted for the fact that the effect of high cholesterol becomes less important for older patients compared to younger patients. .",
            "url": "https://kirankamatmgm.github.io/fastblog/2020/06/10/Risk-score-model.html",
            "relUrl": "/2020/06/10/Risk-score-model.html",
            "date": " • Jun 10, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://kirankamatmgm.github.io/fastblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://kirankamatmgm.github.io/fastblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kirankamatmgm.github.io/fastblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}