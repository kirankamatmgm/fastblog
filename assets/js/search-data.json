{
  
    
        "post0": {
            "title": "Build and Evaluate a Linear Risk model",
            "content": "Building a risk score model for retinopathy in diabetes patients using logistic regression. . Steps . Data preprocessing Log transformations | Standardization | . | Basic Risk Models Logistic Regression | C-index | Interactions Terms | . | . Diabetic Retinopathy . Retinopathy is an eye condition that causes changes to the blood vessels in the part of the eye called the retina. This often leads to vision changes or blindness. Diabetic patients are known to be at high risk for retinopathy. . Logistic Regression . Logistic regression is used for predicting the probability of a binary outcome. In our case, this would be the probability of having or not having diabetic retinopathy. . import numpy as np import pandas as pd import matplotlib.pyplot as plt . . 1. Load Data . #X_data and y_data are generated files taken from Ai in medicine course X = pd.read_csv(&#39;X_data.csv&#39;,index_col=0) y_df = pd.read_csv(&#39;y_data.csv&#39;,index_col=0) y = y_df[&#39;y&#39;] . X and y are Pandas DataFrames that hold the data for 6,000 diabetic patients. . . 2. Explore the Dataset . The features (X) include the following fields: . Age: (years) | Systolic_BP: Systolic blood pressure (mmHg) | Diastolic_BP: Diastolic blood pressure (mmHg) | Cholesterol: (mg/DL) | . X.head() . Age Systolic_BP Diastolic_BP Cholesterol . 0 77.196340 | 85.288742 | 80.021878 | 79.957109 | . 1 63.529850 | 99.379736 | 84.852361 | 110.382411 | . 2 69.003986 | 111.349455 | 109.850616 | 100.828246 | . 3 82.638210 | 95.056128 | 79.666851 | 87.066303 | . 4 78.346286 | 109.154591 | 90.713220 | 92.511770 | . The target (y) is an indicator of whether or not the patient developed retinopathy. . y = 1 : patient has retinopathy. | y = 0 : patient does not have retinopathy. | . y.head() . 0 1.0 1 1.0 2 1.0 3 1.0 4 1.0 Name: y, dtype: float64 . split the data into train and test sets using a 75/25 split. . For this, we can use the built in function provided by sklearn library. See the documentation for sklearn.model_selection.train_test_split. . from sklearn.model_selection import train_test_split . X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=0) . Plot the histograms of each column of X_train below: . for col in X.columns: X_train_raw.loc[:, col].hist() plt.title(col) plt.show() . As we can see, the distributions have a generally bell shaped distribution, but with slight rightward skew. . We can transform our data to be closer to a normal distribution by removing the skew. One way to remove the skew is by applying the log function to the data. . Let&#39;s plot the log of the feature variables to see that it produces the desired effect. . for col in X_train_raw.columns: np.log(X_train_raw.loc[:, col]).hist() plt.title(col) plt.show() . We can see that the data is more symmetric after taking the log. . . 3. Mean-Normalize the Data . Let&#39;s now transform our data so that the distributions are closer to standard normal distributions. . standardizes the distribution so that for each data point $x$, $$ overline{x} = frac{x - mean(x)}{std(x)}$$ . Pretend that the test data is &quot;unseen&quot; data. This implies that it is unavailable to us for the purpose of preparing our data, and so we do not want to consider it when evaluating the mean and standard deviation that we use in the above equation. Instead we want to calculate these values using the training data alone, but then use them for standardizing both the training and the test data. | . | . def make_standard_normal(df_train, df_test): &quot;&quot;&quot; Args: df_train (dataframe): unnormalized training data. df_test (dataframe): unnormalized test data. Returns: df_train_normalized (dateframe): normalized training data. df_test_normalized (dataframe): normalized test data. &quot;&quot;&quot; df_train_unskewed = np.log(df_train) df_test_unskewed = np.log(df_test) mean = df_train_unskewed.mean(axis=0) stdev = df_train_unskewed.std(axis=0,ddof=1) df_train_standardized = (df_train_unskewed - mean)/stdev df_test_standardized = (df_test_unskewed - mean)/stdev return df_train_standardized, df_test_standardized . Transform training and test data . Use the function to make the data distribution closer to a standard normal distribution. . X_train, X_test = make_standard_normal(X_train_raw, X_test_raw) . After transforming the training and test sets, we&#39;ll expect the training set to be centered at zero with a standard deviation of $1$. . We will avoid observing the test set during model training in order to avoid biasing the model training process, but let&#39;s have a look at the distributions of the transformed training data. . for col in X_train.columns: X_train[col].hist() plt.title(col) plt.show() . . 4. Build the Model . Now we are ready to build the risk model by training logistic regression with our data. . def lr_model(X_train, y_train): from sklearn.linear_model import LogisticRegression model = LogisticRegression(solver=&#39;lbfgs&#39;) model.fit(X_train,y_train) return model . model_X = lr_model(X_train, y_train) . . 5. Evaluate the Model Using the C-index . Now that we have a model, we need to evaluate it. We&#39;ll do this using the c-index. . The c-index measures the discriminatory power of a risk score. | Intuitively, a higher c-index indicates that the model&#39;s prediction is in agreement with the actual outcomes of a pair of patients. | The formula for the c-index is | . $$ mbox{cindex} = frac{ mbox{concordant} + 0.5 times mbox{ties}}{ mbox{permissible}} $$ . A permissible pair is a pair of patients who have different outcomes. | A concordant pair is a permissible pair in which the patient with the higher risk score also has the worse outcome. | A tie is a permissible pair where the patients have the same risk score. | . def cindex(y_true, scores): &#39;&#39;&#39; Input: y_true (np.array): a 1-D array of true binary outcomes (values of zero or one) 0: patient does not get the disease 1: patient does get the disease scores (np.array): a 1-D array of corresponding risk scores output by the model Output: c_index (float): (concordant pairs + 0.5*ties) / number of permissible pairs &#39;&#39;&#39; n = len(y_true) assert len(scores) == n concordant = 0 permissible = 0 ties = 0 for i in range(n): for j in range (i+1, n): # Check if the pair is permissible (the patient outcomes are different) if y_true[i]!=y_true[j]: # Count the pair if it&#39;s permissible permissible +=1 # For permissible pairs, check if they are concordant or are ties # check for ties in the score if scores[i]==scores[j]: # count the tie ties+=1 # if it&#39;s a tie, we don&#39;t need to check patient outcomes, continue to the top of the for loop. continue # case 1: patient i doesn&#39;t get the disease, patient j does if y_true[i] == 0 and y_true[j] == 1: # Check if patient i has a lower risk score than patient j if scores[i]&lt;scores[j]: # count the concordant pair concordant+=1 # Otherwise if patient i has a higher risk score, it&#39;s not a concordant pair. # Already checked for ties earlier # case 2: patient i gets the disease, patient j does not if y_true[i]==1 and y_true[j] == 0: # Check if patient i has a higher risk score than patient j if scores[i]&gt;scores[j]: #count the concordant pair concordant+=1 # Otherwise if patient i has a lower risk score, it&#39;s not a concordant pair. # We already checked for ties earlier # calculate the c-index using the count of permissible pairs, concordant pairs, and tied pairs. c_index = (concordant+(0.5*ties))/permissible return c_index . . 6. Evaluate the Model on the Test Set . Now, you can evaluate your trained model on the test set. . To get the predicted probabilities, we use the predict_proba method. This method will return the result from the model before it is converted to a binary 0 or 1. For each input case, it returns an array of two values which represent the probabilities for both the negative case (patient does not get the disease) and positive case (patient the gets the disease). . scores = model_X.predict_proba(X_test)[:, 1] c_index_X_test = cindex(y_test.values, scores) print(f&quot;c-index on test set is {c_index_X_test:.4f}&quot;) . c-index on test set is 0.8182 . Let&#39;s plot the coefficients to see which variables (patient features) are having the most effect. You can access the model coefficients by using model.coef_ . coeffs = pd.DataFrame(data = model_X.coef_, columns = X_train.columns) coeffs.T.plot.bar(legend=None); . . 7. Improve the Model . You can try to improve your model by including interaction terms. . An interaction term is the product of two variables. For example, if we have data $$ x = [x_1, x_2]$$ | We could add the product so that: $$ hat{x} = [x_1, x_2, x_1*x_2]$$ | . | . def add_interactions(X): &quot;&quot;&quot; Add interaction terms between columns to dataframe. Args: X (dataframe): Original data Returns: X_int (dataframe): Original data with interaction terms appended. &quot;&quot;&quot; features = X.columns m = len(features) X_int = X.copy(deep=True) # &#39;i&#39; loops through all features in the original dataframe X for i in range(m): # get the name of feature &#39;i&#39; feature_i_name = features[i] # get the data for feature &#39;i&#39; feature_i_data = X[feature_i_name] # choose the index of column &#39;j&#39; to be greater than column i for j in range (i+1, m): # get the name of feature &#39;j&#39; feature_j_name = features[j] # get the data for feature j&#39; feature_j_data = X[feature_j_name] # create the name of the interaction feature by combining both names # example: &quot;apple&quot; and &quot;orange&quot; are combined to be &quot;apple_x_orange&quot; feature_i_j_name = f&quot;{feature_i_name}_x_{feature_j_name}&quot; # Multiply the data for feature &#39;i&#39; and feature &#39;j&#39; # store the result as a column in dataframe X_int X_int[feature_i_j_name] = X_int[feature_i_name] * X_int[feature_j_name] return X_int . print(&quot;Original Data&quot;) print(X_train.loc[:, [&#39;Age&#39;, &#39;Systolic_BP&#39;]].head()) print(&quot;Data w/ Interactions&quot;) print(add_interactions(X_train.loc[:, [&#39;Age&#39;, &#39;Systolic_BP&#39;]].head())) . Original Data Age Systolic_BP 1824 -0.912451 -0.068019 253 -0.302039 1.719538 1114 2.576274 0.155962 3220 1.163621 -2.033931 2108 -0.446238 -0.054554 Data w/ Interactions Age Systolic_BP Age_x_Systolic_BP 1824 -0.912451 -0.068019 0.062064 253 -0.302039 1.719538 -0.519367 1114 2.576274 0.155962 0.401800 3220 1.163621 -2.033931 -2.366725 2108 -0.446238 -0.054554 0.024344 . Once you have correctly implemented add_interactions, use it to make transformed version of X_train and X_test. . X_train_int = add_interactions(X_train) X_test_int = add_interactions(X_test) . . 8. Evaluate the Improved Model . Now we can train the new and improved version of the model. . model_X_int = lr_model(X_train_int, y_train) . Let&#39;s evaluate our new model on the test set. . scores_X = model_X.predict_proba(X_test)[:, 1] c_index_X_int_test = cindex(y_test.values, scores_X) scores_X_int = model_X_int.predict_proba(X_test_int)[:, 1] c_index_X_int_test = cindex(y_test.values, scores_X_int) print(f&quot;c-index on test set without interactions is {c_index_X_test:.4f}&quot;) print(f&quot;c-index on test set with interactions is {c_index_X_int_test:.4f}&quot;) . c-index on test set without interactions is 0.8182 c-index on test set with interactions is 0.8281 . You should see that the model with interaction terms performs a bit better than the model without interactions. . Now let&#39;s take another look at the model coefficients to try and see which variables made a difference. Plot the coefficients and report which features seem to be the most important. . int_coeffs = pd.DataFrame(data = model_X_int.coef_, columns = X_train_int.columns) int_coeffs.T.plot.bar(); . You may notice that Age, Systolic_BP, and Cholesterol have a positive coefficient. This means that a higher value in these three features leads to a higher prediction probability for the disease. You also may notice that the interaction of Age x Cholesterol has a negative coefficient. This means that a higher value for the Age x Cholesterol product reduces the prediction probability for the disease. . To understand the effect of interaction terms, let&#39;s compare the output of the model we&#39;ve trained on sample cases with and without the interaction. . index = index = 3432 case = X_train_int.iloc[index, :] print(case) . Age 2.502061 Systolic_BP 1.713547 Diastolic_BP 0.268265 Cholesterol 2.146349 Age_x_Systolic_BP 4.287400 Age_x_Diastolic_BP 0.671216 Age_x_Cholesterol 5.370296 Systolic_BP_x_Diastolic_BP 0.459685 Systolic_BP_x_Cholesterol 3.677871 Diastolic_BP_x_Cholesterol 0.575791 Name: 5970, dtype: float64 . We can see that they have above average Age and Cholesterol. We can now see what our original model would have output by zero-ing out the value for Cholesterol and Age. . new_case = case.copy(deep=True) new_case.loc[&quot;Age_x_Cholesterol&quot;] = 0 new_case . Age 2.502061 Systolic_BP 1.713547 Diastolic_BP 0.268265 Cholesterol 2.146349 Age_x_Systolic_BP 4.287400 Age_x_Diastolic_BP 0.671216 Age_x_Cholesterol 0.000000 Systolic_BP_x_Diastolic_BP 0.459685 Systolic_BP_x_Cholesterol 3.677871 Diastolic_BP_x_Cholesterol 0.575791 Name: 5970, dtype: float64 . print(f&quot;Output with interaction: t{model_X_int.predict_proba([case.values])[:, 1][0]:.4f}&quot;) print(f&quot;Output without interaction: t{model_X_int.predict_proba([new_case.values])[:, 1][0]:.4f}&quot;) . Output with interaction: 0.9448 Output without interaction: 0.9965 . We see that the model is less confident in its prediction with the interaction term than without (the prediction value is lower when including the interaction term). With the interaction term, the model has adjusted for the fact that the effect of high cholesterol becomes less important for older patients compared to younger patients. .",
            "url": "https://kirankamatmgm.github.io/fastblog/2020/06/10/Risk-score-model.html",
            "relUrl": "/2020/06/10/Risk-score-model.html",
            "date": " • Jun 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://kirankamatmgm.github.io/fastblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://kirankamatmgm.github.io/fastblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://kirankamatmgm.github.io/fastblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kirankamatmgm.github.io/fastblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}